# Fairness Evaluation and Testing Repository with Streamlit UI

Automated decision-making systems can potentially introduce biases, raising ethical concerns. This has led to the
development of numerous bias mitigation techniques.
However, the selection of a fairness-aware model for a specific dataset often involves a process of trial and error, as
it is not always feasible to predict in advance whether the mitigation measures provided by the model will meet the
user's requirements, or what impact these measures will have on other model metrics such as accuracy and run time.

Existing fairness toolkits lack a comprehensive benchmarking framework. To bridge this gap, we present FairnessEval, a
framework specifically designed to evaluate fairness in Machine Learning models. FairnessEval streamlines dataset preparation,
fairness evaluation, and result presentation, while also offering customization options.
In this demonstration, we highlight the functionality of FairnessEval in the selection and validation of fairness-aware models.
We compare various approaches and simulate deployment scenarios to showcase FairnessEval effectiveness.


# How to use
Download the project from GitHub and open it in the development enviroment you prefer.
open the terminal and write:
"cd src/fairnesseval/graphic"
Now write:
"run streamlit page_welcome.py"
A page on your local host should be opened: you can start to use Fairnesseval.


